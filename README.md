# ğŸŒŸ Bangla RAG System

A **Bangla Retrieval-Augmented Generation (RAG)** system designed for **deep semantic question answering** in Bangla and English. This project uses OCR, chunking, embeddings, and a FastAPI backend to provide context-aware answers to Bangla questions.

---

## ğŸ§± Project Structure

```
bangla-rag/
â”‚
â”œâ”€â”€ api_routes/
â”‚   â”œâ”€â”€ interference.py
â”‚   â””â”€â”€ inference.py
â”‚
â”œâ”€â”€ data_preprocessing/
â”‚   â”œâ”€â”€ extract.py
â”‚   â””â”€â”€ preprocess_data.py
â”‚
â”œâ”€â”€ rag_pipeline/
â”‚   â””â”€â”€ rag.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ store.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md

```

---

## ğŸ§µ Step 1: Environment Setup

### âœ… Install Python Dependencies

Make sure you have **Python 3.8+** installed.

```bash
pip install -r requirements.txt
```

---

## ğŸ¤– Step 2: Install Ollama & Load Model

1. Download and install Ollama: [https://ollama.ai](https://ollama.ai)
2. Pull the required model:

```bash
ollama pull aya-expanse:8b
```

---

## ğŸš€ Step 3: Run the API Server

Start the FastAPI development server:

```bash
uvicorn main:app --reload --port 8000
```

Visit the server at: [http://127.0.0.1:8000](http://127.0.0.1:8000)

### âœ… API Endpoint:

**`POST /api/ask`**

---

## ğŸ“„ API Usage Example

### Headers:

```http
Content-Type: application/json
```

### Request Body (Bangla Question):

```json
{
  "question": "à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à§Ÿà§‡à¦›à§‡?"
}
```

### Response:

```json
{
  "answer": "à¦®à¦¾à¦®à¦¾à¦•à§‡",
  "language prompted": "à¦¬à¦¾à¦‚à¦²à¦¾",
  "confidence": 1.0
}
```

### Request Body (Another Bangla Question):

```json
{
  "question": "à¦¬à¦¿à§Ÿà§‡à¦° à¦¸à¦®à§Ÿ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à§Ÿà¦¸ à¦•à¦¤ à¦›à¦¿à¦²?"
}
```

### Response:

```json
{
  "answer": "à§§à§« à¦¬à¦›à¦°",
  "language prompted": "à¦¬à¦¾à¦‚à¦²à¦¾",
  "confidence": 1.0
}
```

### Request Body (English Question):

```json
{
  "question": "What was the age of Kallyani at the time of her marriage?"
}
```

### Response:

```json
{
  "answer": "à§§à§« à¦¬à¦›à¦°",
  "language prompted": "English",
  "confidence": 1.0
}
```

---

## ğŸ“¦ Dependencies

```txt
langchain==0.1.0
langchain-community==0.0.10
sentence-transformers==2.2.2
langchain-core==0.1.23
langchain-text-splitters==0.3.8
transformers==4.33.3
torch==2.7.1
torchvision==0.22.1
torchaudio==2.2.0
faiss-cpu==1.7.4
llama-index-core==0.12.52
llama-index-embeddings-huggingface==0.5.5
llama-index-instrumentation==0.3.0
llama-index-llms-ollama==0.6.2
llama-index-workflows==1.1.0
bangla-pdf-ocr==0.1.1
dataclasses-json==0.6.7
langdetect==1.0.9
regex==2024.11.6
nltk==3.9.1
ftfy==6.1.3
fastapi==0.104.0
uvicorn==0.24.0
pydantic==2.5.0
requests==2.32.4
ollama==0.1.7
accelerate==0.21.1
scikit-learn==1.6.1
tqdm==4.67.1
loguru==0.7.0
python-dotenv==1.0.0
jupyter==1.0.0
ipykernel==6.26.0
onnx==1.17.0
huggingface_hub==0.21.4
numpy==1.23.5
```

---

## ğŸ§  Technical Design Decisions

### â“ What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?

I used the **`bangla_pdf_ocr`** library for extracting text from scanned PDF documents. I chose it because it provides strong OCR capabilities specifically tuned for Bangla script. I faced several challenges such as inconsistent line breaks, mixed punctuation, and noisy formatting, which I addressed through normalization and preprocessing steps.

### â“ What chunking strategy did you choose (e.g. paragraph-based, sentence-based, character limit)? Why do you think it works well for semantic retrieval?

I use **sentence-based dynamic chunking** with a max token limit (default 512). I merge sentences without exceeding the limit, ensuring semantic coherence. This works well because it preserves full ideas and avoids cutting context mid-thought, which improves retrieval accuracy.

### â“ What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?

I use a **multilingual SentenceTransformer** model from HuggingFace/Ollama. I chose it for its proven performance on semantic tasks across languages. It encodes text into dense vector representations that preserve meaning, not just syntax, which is crucial for Bangla-English mixed input.

### â“ How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?

I compare query and chunk embeddings using **cosine similarity**, stored and indexed via **FAISS**. Cosine similarity measures angle rather than magnitude, making it ideal for semantic search. FAISS gives fast, scalable indexing with IVF or flat modes.

### â“ How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?

I prefix input with semantic tags (e.g., `query:`, `passage:`), perform language detection, and ensure Bangla-aware sentence segmentation. If a query lacks context, the system still returns best matches but with lower confidence. To handle vagueness, I plan to add query clarification logic.

### â“ Do the results seem relevant? If not, what might improve them (e.g. better chunking, better embedding model, larger document)?

Yes, for most clear Bangla questions, results are accurate. But I can improve further with:

* Overlapping chunks (sliding window)
* Stronger, larger multilingual embedding models
* Re-ranking techniques
* Enriching the dataset
* Self-refinement feedback loops

---

## âœ¨ Author

**FAHAD MOHAMMAD REJWANUL ISLAM**.
